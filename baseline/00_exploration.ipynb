{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c3ed64b3-bfcd-4201-adcf-660c7f1a9391",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this one to test the dgl \n",
    "import sys\n",
    "sys.path.append(\"/home/n/nguyenpk/CS6208/GNN_ERC/baseline/DialogueGCN-mianzhang\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb892f77-3401-4bd1-86fd-49c7da55c7ce",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d4151712-4bab-46f7-95fa-916acd8664c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgcn\n",
    "\n",
    "log = dgcn.utils.get_logger()\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, tag_size, args):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.emotion_att = MaskedEmotionAtt(input_dim)\n",
    "        self.lin1 = nn.Linear(input_dim, hidden_size)\n",
    "        self.drop = nn.Dropout(args.drop_rate)\n",
    "        self.lin2 = nn.Linear(hidden_size, tag_size)\n",
    "        if args.class_weight:\n",
    "            self.loss_weights = torch.tensor([1 / 0.086747, 1 / 0.144406, 1 / 0.227883,\n",
    "                                              1 / 0.160585, 1 / 0.127711, 1 / 0.252668]).to(args.device)\n",
    "            self.nll_loss = nn.NLLLoss(self.loss_weights)\n",
    "        else:\n",
    "            self.nll_loss = nn.NLLLoss()\n",
    "\n",
    "    def get_prob(self, h, text_len_tensor):\n",
    "        # h_hat = self.emotion_att(h, text_len_tensor)\n",
    "        # hidden = self.drop(F.relu(self.lin1(h_hat)))\n",
    "        hidden = self.drop(F.relu(self.lin1(h)))\n",
    "        scores = self.lin2(hidden)\n",
    "        log_prob = F.log_softmax(scores, dim=-1)\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def forward(self, h, text_len_tensor):\n",
    "        log_prob = self.get_prob(h, text_len_tensor)\n",
    "        y_hat = torch.argmax(log_prob, dim=-1)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def get_loss(self, h, label_tensor, text_len_tensor):\n",
    "        log_prob = self.get_prob(h, text_len_tensor)\n",
    "        loss = self.nll_loss(log_prob, label_tensor)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class MaskedEmotionAtt(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(MaskedEmotionAtt, self).__init__()\n",
    "        self.lin = nn.Linear(input_dim, input_dim)\n",
    "\n",
    "    def forward(self, h, text_len_tensor):\n",
    "        batch_size = text_len_tensor.size(0)\n",
    "        x = self.lin(h)  # [node_num, H]\n",
    "        ret = torch.zeros_like(h)\n",
    "        s = 0\n",
    "        for bi in range(batch_size):\n",
    "            cur_len = text_len_tensor[bi].item()\n",
    "            y = x[s: s + cur_len]\n",
    "            z = h[s: s + cur_len]\n",
    "            scores = torch.mm(z, y.t())  # [L, L]\n",
    "            probs = F.softmax(scores, dim=1)\n",
    "            out = z.unsqueeze(0) * probs.unsqueeze(-1)  # [1, L, H] x [L, L, 1] --> [L, L, H]\n",
    "            out = torch.sum(out, dim=1)  # [L, H]\n",
    "            ret[s: s + cur_len, :] = out\n",
    "            s += cur_len\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de61dd48-6556-42c8-912e-9b79696ad053",
   "metadata": {},
   "source": [
    "### EdgeAtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b290de07-5353-4a16-b0c7-3c622c8ce937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import dgcn\n",
    "\n",
    "log = dgcn.utils.get_logger()\n",
    "\n",
    "\n",
    "class EdgeAtt(nn.Module):\n",
    "\n",
    "    def __init__(self, g_dim, args):\n",
    "        super(EdgeAtt, self).__init__()\n",
    "        self.device = args.device\n",
    "        self.wp = args.wp\n",
    "        self.wf = args.wf\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros((g_dim, g_dim)).float(), requires_grad=True)\n",
    "        var = 2. / (self.weight.size(0) + self.weight.size(1))\n",
    "        self.weight.data.normal_(0, var)\n",
    "\n",
    "    def forward(self, node_features, text_len_tensor, edge_ind):\n",
    "        batch_size, mx_len = node_features.size(0), node_features.size(1)\n",
    "        alphas = []\n",
    "\n",
    "        weight = self.weight.unsqueeze(0).unsqueeze(0)\n",
    "        att_matrix = torch.matmul(weight, node_features.unsqueeze(-1)).squeeze(-1)  # [B, L, D_g]\n",
    "        for i in range(batch_size):\n",
    "            cur_len = text_len_tensor[i].item()\n",
    "            alpha = torch.zeros((mx_len, 110)).to(self.device)\n",
    "            for j in range(cur_len):\n",
    "                s = j - self.wp if j - self.wp >= 0 else 0\n",
    "                e = j + self.wf if j + self.wf <= cur_len - 1 else cur_len - 1\n",
    "                tmp = att_matrix[i, s: e + 1, :]  # [L', D_g]\n",
    "                feat = node_features[i, j]  # [D_g]\n",
    "                score = torch.matmul(tmp, feat)\n",
    "                probs = F.softmax(score, dim=1)  # [L']\n",
    "                alpha[j, s: e + 1] = probs\n",
    "            alphas.append(alpha)\n",
    "\n",
    "        return alphas\n",
    "\n",
    "# class EdgeAtt(nn.Module):\n",
    "#\n",
    "#     def __init__(self, g_dim, args):\n",
    "#         super(EdgeAtt, self).__init__()\n",
    "#         self.device = args.device\n",
    "#         self.wp = args.wp\n",
    "#         self.wf = args.wf\n",
    "#         self.lin = nn.Linear(g_dim, 110)\n",
    "#\n",
    "#     def forward(self, node_features, text_len_tensor, edge_ind):\n",
    "#         h = self.lin(node_features)  # [B, L, mx]\n",
    "#         alphas = F.softmax(h, dim=-1)\n",
    "#         # alphas = torch.ones((node_features.size(0), node_features.size(1), 110))\n",
    "#         return alphas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9a0fe-7272-4faa-84a2-e688d5407642",
   "metadata": {},
   "source": [
    "### GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3cc48af5-7a1f-4464-b745-01420c073264",
   "metadata": {},
   "outputs": [],
   "source": [
    "### original"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac8286f9-e850-4b1c-8d3c-113f0bd47739",
   "metadata": {},
   "source": [
    "import torch.nn as nn\n",
    "# from dgl.nn.pytorch import RelGraphConv as RGCNConv\n",
    "# from dgl.nn.pytorch import GraphConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "def uniform(size, tensor):\n",
    "    bound = 1.0 / math.sqrt(size)\n",
    "    if tensor is not None:\n",
    "        tensor.data.uniform_(-bound, bound)\n",
    "\n",
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, num_bases, dropout):\n",
    "        super(RGCN, self).__init__()\n",
    "\n",
    "        self.entity_embedding = nn.Embedding(num_entities, 100)\n",
    "        self.relation_embedding = nn.Parameter(torch.Tensor(num_relations, 100))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_embedding, gain=nn.init.calculate_gain('relu'))\n",
    "\n",
    "        self.conv1 = RGCNConv(100, 100, num_relations * 2, num_bases=num_bases)\n",
    "        self.conv2 = RGCNConv(100, 100, num_relations * 2, num_bases=num_bases)\n",
    "        self.dropout_ratio = dropout\n",
    "\n",
    "    def forward(self, entity, edge_index, edge_type, edge_norm):\n",
    "        x = self.entity_embedding(entity)\n",
    "        x = self.conv1(x, edge_index, edge_type, edge_norm)\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_type, edge_norm))\n",
    "        x = F.dropout(x, p = self.dropout_ratio, training = self.training)\n",
    "        x = self.conv2(x, edge_index, edge_type, edge_norm)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def distmult(self, embedding, triplets):\n",
    "        s = embedding[triplets[:,0]]\n",
    "        r = self.relation_embedding[triplets[:,1]]\n",
    "        o = embedding[triplets[:,2]]\n",
    "        score = torch.sum(s * r * o, dim=1)\n",
    "        \n",
    "        return score\n",
    "\n",
    "    def score_loss(self, embedding, triplets, target):\n",
    "        score = self.distmult(embedding, triplets)\n",
    "\n",
    "        return score, F.binary_cross_entropy_with_logits(score, target)\n",
    "\n",
    "    def reg_loss(self, embedding):\n",
    "        return torch.mean(embedding.pow(2)) + torch.mean(self.relation_embedding.pow(2))\n",
    "\n",
    "class RGCNConv(MessagePassing):\n",
    "    r\"\"\"The relational graph convolutional operator from the `\"Modeling\n",
    "    Relational Data with Graph Convolutional Networks\"\n",
    "    <https://arxiv.org/abs/1703.06103>`_ paper\n",
    "    .. math::\n",
    "        \\mathbf{x}^{\\prime}_i = \\mathbf{\\Theta}_{\\textrm{root}} \\cdot\n",
    "        \\mathbf{x}_i + \\sum_{r \\in \\mathcal{R}} \\sum_{j \\in \\mathcal{N}_r(i)}\n",
    "        \\frac{1}{|\\mathcal{N}_r(i)|} \\mathbf{\\Theta}_r \\cdot \\mathbf{x}_j,\n",
    "    where :math:`\\mathcal{R}` denotes the set of relations, *i.e.* edge types.\n",
    "    Edge type needs to be a one-dimensional :obj:`torch.long` tensor which\n",
    "    stores a relation identifier\n",
    "    :math:`\\in \\{ 0, \\ldots, |\\mathcal{R}| - 1\\}` for each edge.\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        num_relations (int): Number of relations.\n",
    "        num_bases (int): Number of bases used for basis-decomposition.\n",
    "        root_weight (bool, optional): If set to :obj:`False`, the layer will\n",
    "            not add transformed root node features to the output.\n",
    "            (default: :obj:`True`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_relations, num_bases,\n",
    "                 root_weight=True, bias=True, **kwargs):\n",
    "        super(RGCNConv, self).__init__(aggr='mean', **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_relations = num_relations\n",
    "        self.num_bases = num_bases\n",
    "\n",
    "        self.basis = nn.Parameter(torch.Tensor(num_bases, in_channels, out_channels))\n",
    "        self.att = nn.Parameter(torch.Tensor(num_relations, num_bases))\n",
    "\n",
    "        if root_weight:\n",
    "            self.root = nn.Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        else:\n",
    "            self.register_parameter('root', None)\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        size = self.num_bases * self.in_channels\n",
    "        uniform(size, self.basis)\n",
    "        uniform(size, self.att)\n",
    "        uniform(size, self.root)\n",
    "        uniform(size, self.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type, edge_norm=None, size=None):\n",
    "        \"\"\"\"\"\"\n",
    "        return self.propagate(edge_index, size=size, x=x, edge_type=edge_type,\n",
    "                              edge_norm=edge_norm)\n",
    "\n",
    "\n",
    "    def message(self, x_j, edge_index_j, edge_type, edge_norm):\n",
    "        w = torch.matmul(self.att, self.basis.view(self.num_bases, -1))\n",
    "\n",
    "        # If no node features are given, we implement a simple embedding\n",
    "        # loopkup based on the target node index and its edge type.\n",
    "        if x_j is None:\n",
    "            w = w.view(-1, self.out_channels)\n",
    "            index = edge_type * self.in_channels + edge_index_j\n",
    "            out = torch.index_select(w, 0, index)\n",
    "        else:\n",
    "            w = w.view(self.num_relations, self.in_channels, self.out_channels)\n",
    "            w = torch.index_select(w, 0, edge_type)\n",
    "            out = torch.bmm(x_j.unsqueeze(1), w).squeeze(-2)\n",
    "\n",
    "        return out if edge_norm is None else out * edge_norm.view(-1, 1)\n",
    "\n",
    "    def update(self, aggr_out, x):\n",
    "        if self.root is not None:\n",
    "            if x is None:\n",
    "                out = aggr_out + self.root\n",
    "            else:\n",
    "                out = aggr_out + torch.matmul(x, self.root)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out = out + self.bias\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, num_relations={})'.format(\n",
    "            self.__class__.__name__, self.in_channels, self.out_channels,\n",
    "            self.num_relations)\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, g_dim, h1_dim, h2_dim, args):\n",
    "        super(GCN, self).__init__()\n",
    "        self.num_relations = 2 * args.n_speakers ** 2\n",
    "        self.conv1 = RGCNConv(g_dim, h1_dim, self.num_relations, num_bases=30)\n",
    "        self.conv2 = GraphConv(h1_dim, h2_dim)\n",
    "\n",
    "    def forward(self, node_features, edge_index, edge_norm, edge_type):\n",
    "        x = self.conv1(node_features, edge_index, edge_type, edge_norm=edge_norm)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6fe76ae3-c78e-4042-bad9-a690a46c98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import dgl\n",
    "from dgl.nn.pytorch import RelGraphConv as RGCNConv\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "class GCN(nn.Module):\n",
    "\n",
    "    def __init__(self, g_dim, h1_dim, h2_dim, args):\n",
    "        super(GCN, self).__init__()\n",
    "        self.num_relations = 2 * args.n_speakers ** 2\n",
    "        self.conv1 = RGCNConv(g_dim, h1_dim, self.num_relations, num_bases=30)\n",
    "        self.conv2 = GraphConv(h1_dim, h2_dim)\n",
    "        if args.device != 'cpu':\n",
    "            self.conv1 = self.conv1.cuda()\n",
    "            self.conv2 = self.conv2.cuda()\n",
    "\n",
    "    def forward(self, node_features, edge_index, edge_norm, edge_type):\n",
    "        g = dgl.graph((edge_index[0], edge_index[1]))\n",
    "        g.norm = edge_norm\n",
    "        x = self.conv1(g, node_features, edge_type)\n",
    "        x = self.conv2(g, x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8497d-63b7-44c1-8765-505cbd0700d7",
   "metadata": {},
   "source": [
    "### SeqContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b1bfb11b-2079-4a9e-a126-b1aa4e21f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "class SeqContext(nn.Module):\n",
    "\n",
    "    def __init__(self, u_dim, g_dim, args):\n",
    "        super(SeqContext, self).__init__()\n",
    "        self.input_size = u_dim\n",
    "        self.hidden_dim = g_dim\n",
    "        if args.rnn == \"lstm\":\n",
    "            self.rnn = nn.LSTM(self.input_size, self.hidden_dim // 2, dropout=args.drop_rate,\n",
    "                               bidirectional=True, num_layers=2, batch_first=True)\n",
    "        elif args.rnn == \"gru\":\n",
    "            self.rnn = nn.GRU(self.input_size, self.hidden_dim // 2, dropout=args.drop_rate,\n",
    "                              bidirectional=True, num_layers=2, batch_first=True)\n",
    "\n",
    "    def forward(self, text_len_tensor, text_tensor):\n",
    "        packed = pack_padded_sequence(\n",
    "            text_tensor,\n",
    "            text_len_tensor.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False\n",
    "        )\n",
    "        # print(len(self.rnn(packed, None)))\n",
    "        # rnn_out, (_, _) = self.rnn(packed, None)\n",
    "        rnn_out,  _ = self.rnn(packed, None)\n",
    "        rnn_out, _ = pad_packed_sequence(rnn_out, batch_first=True)\n",
    "\n",
    "        return rnn_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5063e712-b4d4-4fcc-bf11-46efeabf4956",
   "metadata": {},
   "source": [
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "934d2023-aa8b-4111-b779-263dcb14fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import dgcn\n",
    "\n",
    "log = dgcn.utils.get_logger()\n",
    "\n",
    "\n",
    "def batch_graphify(features, lengths, speaker_tensor, wp, wf, edge_type_to_idx, att_model, device):\n",
    "    node_features, edge_index, edge_norm, edge_type = [], [], [], []\n",
    "    batch_size = features.size(0)\n",
    "    length_sum = 0\n",
    "    edge_ind = []\n",
    "    edge_index_lengths = []\n",
    "\n",
    "    for j in range(batch_size):\n",
    "        edge_ind.append(edge_perms(lengths[j].cpu().item(), wp, wf))\n",
    "\n",
    "    edge_weights = att_model(features, lengths, edge_ind)\n",
    "\n",
    "    for j in range(batch_size):\n",
    "        cur_len = lengths[j].item()\n",
    "        node_features.append(features[j, :cur_len, :])\n",
    "        perms = edge_perms(cur_len, wp, wf)\n",
    "        perms_rec = [(item[0] + length_sum, item[1] + length_sum) for item in perms]\n",
    "        length_sum += cur_len\n",
    "        edge_index_lengths.append(len(perms))\n",
    "\n",
    "        for item, item_rec in zip(perms, perms_rec):\n",
    "            edge_index.append(torch.tensor([item_rec[0], item_rec[1]]))\n",
    "            edge_norm.append(edge_weights[j][item[0], item[1]])\n",
    "            # edge_norm.append(edge_weights[j, item[0], item[1]])\n",
    "\n",
    "            speaker1 = speaker_tensor[j, item[0]].item()\n",
    "            speaker2 = speaker_tensor[j, item[1]].item()\n",
    "            if item[0] < item[1]:\n",
    "                c = '0'\n",
    "            else:\n",
    "                c = '1'\n",
    "            edge_type.append(edge_type_to_idx[str(speaker1) + str(speaker2) + c])\n",
    "\n",
    "    node_features = torch.cat(node_features, dim=0).to(device)  # [E, D_g]\n",
    "    edge_index = torch.stack(edge_index).t().contiguous().to(device)  # [2, E]\n",
    "    edge_norm = torch.stack(edge_norm).to(device)  # [E]\n",
    "    edge_type = torch.tensor(edge_type).long().to(device)  # [E]\n",
    "    edge_index_lengths = torch.tensor(edge_index_lengths).long().to(device)  # [B]\n",
    "\n",
    "    graph_out = dgl.graph((edge_index[0], edge_index[1]))\n",
    "    graph_out.norm = edge_norm\n",
    "    graph_out.ndata['feat'] = node_features\n",
    "    graph_out.ndata['PE'] = dgl.laplacian_pe(graph_out, k=pos_enc_size, padding=True).to(device)\n",
    "    graph_out.edata['feat'] = edge_norm\n",
    "    graph_out.edge_index_lengths = edge_index_lengths\n",
    "    return graph_out\n",
    "\n",
    "\n",
    "def edge_perms(length, window_past, window_future):\n",
    "    \"\"\"\n",
    "    Method to construct the edges of a graph (a utterance) considering the past and future window.\n",
    "    return: list of tuples. tuple -> (vertice(int), neighbor(int))\n",
    "    \"\"\"\n",
    "\n",
    "    all_perms = set()\n",
    "    array = np.arange(length)\n",
    "    for j in range(length):\n",
    "        perms = set()\n",
    "\n",
    "        if window_past == -1 and window_future == -1:\n",
    "            eff_array = array\n",
    "        elif window_past == -1:  # use all past context\n",
    "            eff_array = array[:min(length, j + window_future + 1)]\n",
    "        elif window_future == -1:  # use all future context\n",
    "            eff_array = array[max(0, j - window_past):]\n",
    "        else:\n",
    "            eff_array = array[max(0, j - window_past):min(length, j + window_future + 1)]\n",
    "\n",
    "        for item in eff_array:\n",
    "            perms.add((j, item))\n",
    "        all_perms = all_perms.union(perms)\n",
    "    return list(all_perms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e9405-9911-4baf-bce3-573fbb2a2307",
   "metadata": {},
   "source": [
    "## Read data and check training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d14ac5-cc0b-4c1e-8f51-92a55c83e608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6526e4-9165-4966-b8f4-e6e5f89c9419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66cfd0-f54f-4fa0-bfeb-a8589b71f55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "98a53e41-8ba6-42d4-ba40-f23d20951aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_features = model.rnn(data[\"text_len_tensor\"], data[\"text_tensor\"]) # [batch_size, mx_len, D_g]\n",
    "# features, edge_index, edge_norm, edge_type, edge_index_lengths = batch_graphify(\n",
    "#     node_features, data[\"text_len_tensor\"], data[\"speaker_tensor\"], args.wp, args.wf,\n",
    "#     model.edge_type_to_idx, model.edge_att, args.device)\n",
    "\n",
    "# # graph_out = self.gcn(features, edge_index, edge_norm, edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "fe0bf54d-2268-49a8-bc56-502e8571c841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dgl\n",
    "# from dgl.nn import RelGraphConv\n",
    "# from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "# g_dim = 200\n",
    "# h1_dim = 100\n",
    "# h2_dim = 100\n",
    "# hc_dim = 100\n",
    "# tag_size = 6\n",
    "\n",
    "\n",
    "# g = dgl.graph((edge_index[0], edge_index[1]))\n",
    "# g.norm = edge_norm\n",
    "# # g.etypes= edge_type\n",
    "# conv = RelGraphConv(g_dim, h1_dim, h2_dim, regularizer='basis', num_bases=30).cuda()\n",
    "# conv1 = GraphConv(h1_dim, h2_dim).cuda()\n",
    "# # res = conv(g, feat, etype)\n",
    "\n",
    "# res = conv(g, features, edge_type)\n",
    "# res2 = conv1(g, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "8aaf191d-107d-428f-8400-0a5450e13dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=1567, num_edges=29387,\n",
       "      ndata_schemes={'feat': Scheme(shape=(200,), dtype=torch.float32), 'PE': Scheme(shape=(2,), dtype=torch.float32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "624d0f44-5399-45e6-8bd9-551aaf247b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "## graph transformer \n",
    "torch.backends.cudnn.enabled=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "3719cb03-14df-4b99-b78a-d29f0241e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.nn as dglnn\n",
    "import dgl.sparse as dglsp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from dgl.data import AsGraphPredDataset\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from ogb.graphproppred import collate_dgl, DglGraphPropPredDataset, Evaluator\n",
    "from ogb.graphproppred.mol_encoder import AtomEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "class MLP_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, L=2): # L = nb of hidden layers\n",
    "        super(MLP_layer, self).__init__()\n",
    "        list_FC_layers = [ nn.Linear( input_dim, input_dim, bias=True ) for l in range(L) ]\n",
    "        list_FC_layers.append(nn.Linear( input_dim, output_dim , bias=True ))\n",
    "        self.FC_layers = nn.ModuleList(list_FC_layers)\n",
    "        self.L = L\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for l in range(self.L):\n",
    "            y = self.FC_layers[l](y)\n",
    "            y = torch.relu(y)\n",
    "        y = self.FC_layers[self.L](y)\n",
    "        return y\n",
    "\n",
    "class SparseMHA(nn.Module):\n",
    "    \"\"\"Sparse Multi-head Attention Module\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=80, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_size // num_heads\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, A, h):\n",
    "        N = len(h)\n",
    "        # [N, dh, nh]\n",
    "        q = self.q_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
    "        q *= self.scaling\n",
    "        # [N, dh, nh]\n",
    "        k = self.k_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
    "        # [N, dh, nh]\n",
    "        v = self.v_proj(h).reshape(N, self.head_dim, self.num_heads)\n",
    "\n",
    "        ######################################################################\n",
    "        # (HIGHLIGHT) Compute the multi-head attention with Sparse Matrix API\n",
    "        ######################################################################\n",
    "        attn = dglsp.bsddmm(A, q, k.transpose(1, 0))  # (sparse) [N, N, nh]\n",
    "        # Sparse softmax by default applies on the last sparse dimension.\n",
    "        attn = attn.softmax()  # (sparse) [N, N, nh]\n",
    "        out = dglsp.bspmm(attn, v)  # [N, dh, nh]\n",
    "\n",
    "        return self.out_proj(out.reshape(N, -1))\n",
    "    \n",
    "class GTLayer(nn.Module):\n",
    "    \"\"\"Graph Transformer Layer\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size=80, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.MHA = SparseMHA(hidden_size=hidden_size, num_heads=num_heads)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.FFN1 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.FFN2 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, A, h):\n",
    "        h1 = h\n",
    "        h = self.MHA(A, h)\n",
    "        h = self.batchnorm1(h + h1)\n",
    "\n",
    "        h2 = h\n",
    "        h = self.FFN2(F.relu(self.FFN1(h)))\n",
    "        h = h2 + h\n",
    "\n",
    "        return self.batchnorm2(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "0aad5b50-d6e6-4271-bb0c-e70dbb9a961c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        out_size, # 6\n",
    "        input_size=200, # g_dim\n",
    "        hidden_size=80,\n",
    "        pos_enc_size=2,\n",
    "        num_layers=8,\n",
    "        num_heads=8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_h =  nn.Linear(input_dim, hidden_size)#dgl.nn.GATConv(input_dim, hidden_size, num_heads=num_heads)\n",
    "        self.pos_linear = nn.Linear(pos_enc_size, hidden_size)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [GTLayer(hidden_size, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.predictor = MLP_layer(hidden_size, out_size)\n",
    "\n",
    "    def forward(self, g, X, pos_enc):\n",
    "        indices = torch.stack(g.edges())\n",
    "        N = g.num_nodes()\n",
    "        A = dglsp.spmatrix(indices, shape=(N, N))\n",
    "\n",
    "        h = self.embedding_h(X) + self.pos_linear(pos_enc)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h = layer(A, h)\n",
    "        return self.predictor(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "565ad31c-5bef-4c62-a2cd-51b45175ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogueGCN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(DialogueGCN, self).__init__()\n",
    "        u_dim = 100\n",
    "        g_dim = 200\n",
    "        # h1_dim = 100\n",
    "        # h2_dim = 100\n",
    "        # hc_dim = 100\n",
    "        tag_size = 6\n",
    "\n",
    "        self.wp = args.wp\n",
    "        self.wf = args.wf\n",
    "        self.device = args.device\n",
    "        #\n",
    "        self.rnn = SeqContext(u_dim, g_dim, args)\n",
    "        self.edge_att = EdgeAtt(g_dim, args)\n",
    "        self.gtm = GTModel(tag_size, input_size= g_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        edge_type_to_idx = {}\n",
    "        for j in range(args.n_speakers):\n",
    "            for k in range(args.n_speakers):\n",
    "                edge_type_to_idx[str(j) + str(k) + '0'] = len(edge_type_to_idx)\n",
    "                edge_type_to_idx[str(j) + str(k) + '1'] = len(edge_type_to_idx)\n",
    "        self.edge_type_to_idx = edge_type_to_idx\n",
    "        log.debug(self.edge_type_to_idx)\n",
    "\n",
    "    def get_rep(self, data):\n",
    "        node_features = self.rnn(data[\"text_len_tensor\"], data[\"text_tensor\"]) # [batch_size, mx_len, D_g]\n",
    "        graph_out = batch_graphify(\n",
    "            node_features, data[\"text_len_tensor\"], data[\"speaker_tensor\"], self.wp, self.wf,\n",
    "            self.edge_type_to_idx, self.edge_att, self.device)\n",
    "        return graph_out\n",
    "\n",
    "    def forward(self, data):\n",
    "        graph_out= self.get_rep(data)\n",
    "        out = self.gtm(graph_out, graph_out.ndata['feat'], graph_out.ndata['PE'])\n",
    "        # out = torch.argmax(out, dim=-1)\n",
    "        return self.softmax(out)\n",
    "\n",
    "#     def loss(self, y_scores, y_labels):\n",
    "#         loss = nn.CrossEntropyLoss()(y_scores, y_labels)\n",
    "#         return loss        \n",
    "        \n",
    "#     def accuracy(self, scores, targets):\n",
    "#         scores = scores.detach().argmax(dim=1)\n",
    "#         acc = (scores==targets).float().sum().item()\n",
    "#         return acc\n",
    "    \n",
    "#     def update(self, lr):       \n",
    "#         update = torch.optim.Adam( self.parameters(), lr=lr )\n",
    "#         return update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "02b1568b-1bca-48c9-8862-acb60c6cbc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = \"/home/n/nguyenpk/CS6208/GNN_ERC/baseline/DialogueGCN-mianzhang\"\n",
    "data_path = os.path.join(base_path, \"data/iemocap/ckpt/data.pkl\")\n",
    "batch_size = 32\n",
    "device  = \"cuda:0\"\n",
    "learning_rate = 0.0003\n",
    "max_grad_value = -1\n",
    "weight_decay = 1e-8\n",
    "optimizer = \"adam\"\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Namespace(batch_size=batch_size, \n",
    "                 device=device,\n",
    "                 learning_rate=learning_rate,\n",
    "                 max_grad_value=max_grad_value, \n",
    "                 weight_decay=weight_decay, \n",
    "                 optimizer=optimizer, \n",
    "                 from_begin=True,\n",
    "                 epochs=20,\n",
    "                 drop_rate=0.5,\n",
    "                 wp=10,\n",
    "                 wf=10,\n",
    "                 n_speakers=2,\n",
    "                 hidden_size=100,\n",
    "                 rnn='gru',\n",
    "                 class_weight=True,\n",
    "                 seed=24,\n",
    "                 \n",
    ")\n",
    "data = dgcn.utils.load_pkl(data_path)\n",
    "trainset = dgcn.Dataset(data[\"train\"], args.batch_size)\n",
    "devset = dgcn.Dataset(data[\"dev\"], args.batch_size)\n",
    "testset = dgcn.Dataset(data[\"test\"], args.batch_size)\n",
    "\n",
    "model_file = \"./save/model.pt\"\n",
    "model = DialogueGCN(args).to(device)\n",
    "opt = dgcn.Optim(learning_rate, max_grad_value, weight_decay)\n",
    "opt.set_parameters(model.parameters(), optimizer)\n",
    "\n",
    "label_to_idx = {'hap': 0, 'sad': 1, 'neu': 2, 'ang': 3, 'exc': 4, 'fru': 5}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "226b1eb7-898b-4ebc-aa89-79fdd37b69d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:18:34 [Epochs 0: , loss: 1.5844167470932007,  f1_train:0.5794932159346788, f1_test: 0.4080484277190507]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:19:03 [Epochs 1: , loss: 0.9388782382011414,  f1_train:0.6339281738304317, f1_test: 0.5848785369892672]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:19:31 [Epochs 2: , loss: 0.6961487531661987,  f1_train:0.6862320217087663, f1_test: 0.6011949254569395]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:20:00 [Epochs 3: , loss: 0.5864019989967346,  f1_train:0.7321332469023084, f1_test: 0.5571674552689915]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:20:29 [Epochs 4: , loss: 0.4575531482696533,  f1_train:0.8180331575483564, f1_test: 0.571455294890675]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:20:57 [Epochs 5: , loss: 0.3425407111644745,  f1_train:0.871577075894544, f1_test: 0.5755200905115831]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:21:27 [Epochs 6: , loss: 0.21159973740577698,  f1_train:0.9027469924387953, f1_test: 0.5536779697462095]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:21:56 [Epochs 7: , loss: 0.15840855240821838,  f1_train:0.9225289329250447, f1_test: 0.5478835015279871]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:22:24 [Epochs 8: , loss: 0.1295231729745865,  f1_train:0.9290618723851851, f1_test: 0.5304414598558329]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:22:53 [Epochs 9: , loss: 0.08705036342144012,  f1_train:0.9401985142122491, f1_test: 0.5434787937526424]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:23:21 [Epochs 10: , loss: 0.07253216207027435,  f1_train:0.9470787368706426, f1_test: 0.5499216671191951]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:23:50 [Epochs 11: , loss: 0.05674681439995766,  f1_train:0.9516240612136447, f1_test: 0.5568869362142781]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:24:20 [Epochs 12: , loss: 0.04587560519576073,  f1_train:0.9595468276949358, f1_test: 0.5517029088932707]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:24:47 [Epochs 13: , loss: 0.03575770929455757,  f1_train:0.9658807524326024, f1_test: 0.5552729993697084]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:25:17 [Epochs 14: , loss: 0.028528830036520958,  f1_train:0.9698377605558662, f1_test: 0.5523048972624998]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:25:45 [Epochs 15: , loss: 0.0186869278550148,  f1_train:0.9756599078106937, f1_test: 0.5436536393286213]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:26:14 [Epochs 16: , loss: 0.014933847822248936,  f1_train:0.9788079377035477, f1_test: 0.5467255058178825]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:26:43 [Epochs 17: , loss: 0.009903491474688053,  f1_train:0.9805372607773785, f1_test: 0.5508320618659671]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:27:11 [Epochs 18: , loss: 0.008538507856428623,  f1_train:0.9808075229708823, f1_test: 0.5436553776391468]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:27:40 [Epochs 19: , loss: 0.006060297600924969,  f1_train:0.9832798747083804, f1_test: 0.5512229549189175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n",
      "/tmp/ipykernel_3717044/39771507.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probs = F.softmax(score)  # [L']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/23/2023 03:28:10 [Epochs 20: , loss: 0.005354269873350859,  f1_train:0.980759976253737, f1_test: 0.5381254114917422]\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(filename='train_test.log', encoding='utf-8')\n",
    "def evaluation(model, dataset,  device='cuda:0'):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    logits = []\n",
    "    #-----------------------\n",
    "    with torch.no_grad():\n",
    "        for idx in range(len(dataset)):\n",
    "            data = dataset[idx]\n",
    "            y_true.append(data['label_tensor'])\n",
    "            for k, v in data.items():\n",
    "                data[k] = v.to(device)\n",
    "            logits = model(data)\n",
    "            y_hat = torch.argmax(logits, dim=-1).detach().cpu()\n",
    "            y_pred.append(y_hat)\n",
    "        y_true = torch.cat(y_true, dim=0).numpy()\n",
    "        y_pred = torch.cat(y_pred, dim=0).numpy()\n",
    "        f1  = sklearn.metrics.f1_score(y_true, y_pred,average=\"weighted\")\n",
    "        return f1\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 20\n",
    "scheduler = optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=num_epochs, gamma=0.5\n",
    ")\n",
    "# loss_fcn = nn.CrossEntropyLoss() # Can change as the paper itself\n",
    "# loss_fcn = nn.NLLLoss()\n",
    "loss_weights = torch.tensor([1 / 0.086747, 1 / 0.144406, 1 / 0.227883,\n",
    "                                              1 / 0.160585, 1 / 0.127711, 1 / 0.252668]).to(args.device)\n",
    "loss_fcn = nn.NLLLoss(loss_weights)\n",
    "model.train()\n",
    "\n",
    "best_state = None\n",
    "best_dev_f1 = None\n",
    "best_epoch = None\n",
    "\n",
    "for epoch in range(args.epochs + 1):\n",
    "    total_loss = 0\n",
    "    for idx in range(len(trainset)):\n",
    "        idata = trainset[idx]\n",
    "        label = idata['label_tensor'].to(args.device)\n",
    "        for k, v in idata.items():\n",
    "            idata[k] = v.to(args.device)\n",
    "        logits = model(idata)\n",
    "        loss = loss_fcn(logits, label)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    ev_train = evaluation(model, trainset)\n",
    "    dev1 = ev_train\n",
    "    if best_epoch is None or dev1 > best_dev_f1:\n",
    "        best_dev_f1 = dev1\n",
    "        best_epoch = epoch\n",
    "        best_state = copy.deepcopy(model.state_dict())\n",
    "    ev_test = evaluation(model, testset)\n",
    "    log.info('[Epochs {}: , loss: {},  f1_train:{}, f1_test: {}]'\n",
    "          .format(epoch, loss,  ev_train, ev_test)\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "db085b28-cc03-4a97-bf00-a361bb56df80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.8777e-04, 4.1262e-05, 9.9952e-01, 3.6980e-05, 9.0855e-06, 8.7924e-06],\n",
       "        [3.3682e-10, 9.6676e-12, 9.4629e-06, 7.3109e-10, 2.6886e-06, 9.9999e-01],\n",
       "        [9.2900e-12, 5.1017e-14, 5.5641e-08, 1.5471e-11, 1.2859e-06, 1.0000e+00],\n",
       "        ...,\n",
       "        [1.0000e+00, 2.3056e-10, 1.7881e-07, 2.6601e-06, 3.5811e-07, 1.0478e-10],\n",
       "        [1.0000e+00, 1.9890e-12, 3.0023e-09, 1.5138e-07, 1.3159e-08, 5.1179e-13],\n",
       "        [1.0000e+00, 7.1199e-13, 7.7585e-10, 5.7225e-08, 6.6974e-09, 1.3705e-13]],\n",
       "       device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "ed29d564-1d0e-4b94-9459-a9b00092c2d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "9350de73-f974-435c-b148-1c7c4e1f85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softmax(dim=1)\n",
    ">>> input = torch.randn(2, 3)\n",
    ">>> output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "1ba6e300-effc-46f8-b604-0df3336bfc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2959, 0.4747, 0.2294],\n",
       "        [0.5376, 0.1552, 0.3072]])"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "febfbcb5-7a5b-4137-82fb-a72fc0a98eba",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[371], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m softmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_transform/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_transform/lib/python3.10/site-packages/torch/nn/modules/activation.py:1376\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/graph_transform/lib/python3.10/site-packages/torch/nn/functional.py:1834\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1834\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1836\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 6)"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=6)\n",
    "softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "53d28c8f-7700-4c8d-be8c-00fbe8b9d003",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "# output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "8ed9392d-40dd-46e3-8d78-1c03c693f10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4111,  1.2146, -0.0983, -0.6432,  1.5157],\n",
       "        [ 0.5109,  0.1624,  0.2416, -1.2818, -0.1733],\n",
       "        [ 0.3137, -0.8718,  0.8968,  1.9743, -1.1651]], requires_grad=True)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "d3755e07-2caa-4f8f-9f8b-d84924fa90a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "20cda0a7-d0af-48b2-a9fd-b78c767203ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0764, 0.0910, 0.4077, 0.2993, 0.1256],\n",
       "        [0.0221, 0.0217, 0.1789, 0.0443, 0.7330],\n",
       "        [0.1392, 0.4521, 0.1234, 0.0679, 0.2173]])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 5).softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "b16e59c6-2d22-4c70-9175-970409e8d1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9045, grad_fn=<DivBackward1>)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.CrossEntropyLoss()(input, torch.randn(3, 5).softmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13af16fb-bb4a-4b8f-9f2b-d7430fcafe18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Graph_Transform_GNN",
   "language": "python",
   "name": "graph_transform_gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
